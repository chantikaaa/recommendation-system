# -*- coding: utf-8 -*-
"""model-sistem-rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OuJQ0CTeVsFH_-VF_0z1R-v5qprlSjhU

# **Sistem Rekomendasi Buku menggunakan _Collaborative Filtering_ ðŸ¤“ðŸ“–**

## Import Libraries

Pada tahap ini, diimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model sistem rekomendasi.
"""

import pandas as pd
import os
import numpy as np
import kagglehub
from google.colab import drive
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2

"""## Data Loading

Memuat dataset goodbooks dari Kaggle menggunakan kagglehub. Dataset ini terdiri atas `book_tags.csv`, `books.csv`, `ratings.csv`, `sample_book.xml`, `tags.csv`, dan `to_read.csv`. Namun, pada pembuatan model sistem rekomendasi ini, hanya memanfaatkan dua variabel saja dari dataset tersebut, yaitu `ratings.csv` dan `books.csv` untuk melatih model Collaborative Filtering.
"""

drive.mount('/content/drive')

# Download latest version
path = kagglehub.dataset_download("zygmunt/goodbooks-10k")

print("Path to dataset files:", path)

rating = pd.read_csv("/kaggle/input/goodbooks-10k/ratings.csv")
books = pd.read_csv("/kaggle/input/goodbooks-10k/books.csv",
                 usecols=["book_id",
                          "original_publication_year",
                          "authors",
                          "average_rating",
                          "title"]) # menggunakan fitur yang relevan

print('Jumlah data buku yang tersimpan: ', len(books.book_id.unique()))
print('Jumlah data rating oleh pembaca yang tersimpan: ', rating.shape[0])

books.head()

rating.head(5)

"""## Exploratory Data Analysis (EDA)

Dilakukan Exploratory Data Analysis (EDA) pada dataset dengan tujuan untuk memahami data lebih dalam, menemukan pola tersembunyi, dan mengidentifikasi anomali atau masalah dalam data sebelum membangun model rekomendasi.

### Variabel Books
"""

books.info()

"""Insight : Pada atribut `original_publication_year`, terdapat beberapa data yang termasuk null content. Penggunaan `authors` dan `original_publication_year` ini hanya untuk mengetahui apakah ada duplikasi data buku atau tidak. Namun, untuk model collaborative filtering, fitur ini akan dihilangkan di preprocessing."""

books.shape

books.describe()

"""Insight : Terdapat tahun -1750 pada fitur `original_publication_year`, yang merupakan outlier. Namun, karena fitur ini tidak akan digunakan dalam modelling, maka dapat diabaikan saja."""

# Memeriksa duplikasi data
duplikat_buku = books.duplicated().sum()
print(f"Jumlah baris duplikat: {duplikat_buku}")
# Hanya duplikasi judul
duplikat_judul = books.duplicated(subset='title').sum()
print(f"Jumlah baris duplikat judul: {duplikat_judul}")
# Hanya duplikasi book_id
duplikat_bookid = books.duplicated(subset='book_id').sum()
print(f"Jumlah baris duplikat id buku: {duplikat_bookid}")

"""Insight : Terdapat 36 buah baris duplikat judul, namun ini bukanlah indikasi data duplikasi karena memungkinkan duplikat judul buku memang sama, namun data atribut/fitur lain berbeda. Untuk memastikan, ditampilkan baris dengan judul yang duplikat seperti di bawah."""

# Menampilkan seluruh baris dengan judul yang duplikat
duplikat_judul_df = books[books.duplicated(subset='title', keep=False)]
duplikat_judul_df.sort_values('title')

# Menampilkan seluruh baris yang memiliki judul yang duplikat
pd.set_option('display.max_rows', None)  # Menampilkan semua baris dan semua kolom
pd.set_option('display.max_columns', None)

# Kemudian tampilkan:
duplikat_judul_df.sort_values('title')

"""Insight : Buku-buku yang memiliki judul sama memiliki author(s) yang berbeda dan tahun publikasi yang berbeda. Maka, data duplikasi ini tidak perlu dihilangkan karena kasus judul buku yang sama memang wajar terjadi."""

# Memeriksa data null
books.isna().sum()

"""Insight : Terdapat 21 data null di original_publication_year, namun data null ini tidak perlu diatasi/dihilangkan, karena fitur ini tidak akan digunakan untuk model, hanya untuk menentukan duplikasi buku."""

# Cek apabila kode book_id berurutan atau memerlukan encoding
print("Max book_id:", rating['book_id'].max())
print("Unique books:", rating['book_id'].nunique())

"""Insight : `book_id` sudah memiliki nilai unique yang sudah berupa integer terurut, sehingga kemungkinan tidak memerlukan proses encoding nantinya."""

# Memeriksa distribusi average rating
books['average_rating'].hist(bins=30)
plt.title('Distribusi Rata-rata Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.show()

"""Insight: Di dataset ini, buku yang di-_rating_ oleh pembaca kebanyakan memiliki rating di sekitar angka 4.0

### Variabel Rating
"""

rating.info()

rating.shape

rating.describe()

# Memeriksa duplikasi data
duplikat_rating = rating.duplicated().sum()
print(f"Jumlah baris duplikat: {duplikat_rating}")
# Hanya duplikasi user_id dan book_id
dupe_count = rating.duplicated(subset=['user_id', 'book_id'], keep=False).sum()
print(f'Jumlah rating duplikat (user-buku): {dupe_count}')

# Tampilkan pasangan user-buku yang punya rating lebih dari satu kali
dupes = rating.groupby(['user_id', 'book_id']).size().reset_index(name='count')
dupes = dupes[dupes['count'] > 1]
print(f'Jumlah kombinasi user-buku yang duplikat: {len(dupes)}')

"""Insight : Terdapat data duplikat yang nantinya akan diatasi di _preprocessing_. Dari hasil 2209 pasangan user-buku unik yang muncul lebih dari sekali, dan total jumlah rating-nya 4487. di atas, dapat disimpulkan bahwa ada user yang memberi rating ke buku yang sama lebih dari dua kali.


"""

# Memeriksa data null
rating.isna().sum()

"""Insight : Tidak terdapat data null yang perlu diatasi"""

# Cek nilai unik di fitur rating (numerik), apakah ada nilai yang melebihi 5
print(rating['rating'].unique())

"""Insight : Tidak terdapat data _outlier_ yang perlu diatasi"""

# Cek perbedaan antara book_id di rating dan books
print("Unique book_id di books:", books['book_id'].nunique())
print("Unique book_id di rating:", rating['book_id'].nunique())

missing_books = set(rating['book_id']) - set(books['book_id'])
print("Jumlah book_id yang hilang:", len(missing_books))

"""Insight : Karena `book_id` yang hilang ada 9188 data, maka `book_id` yang ada di **rating dan books** hanya ada 812 buku. Supaya output rekomendasi yang dihasilkan jelas dan memiliki judul buku, maka sistem rekomendasi akan memanfaatkan 812 buku ini."""

# Mengetahui user_id unik
print(np.sort(rating['user_id'].unique()))

# Mengetahui book_id unik
print((rating['book_id'].unique()))

# Cek apabila nilai  unique sudah urut atau memerlukan encoding
print("Max user_id:", rating['user_id'].max())
print("Unique users:", rating['user_id'].nunique())

print("Max book_id:", rating['book_id'].max())
print("Unique books:", rating['book_id'].nunique())

"""Insight : `user_id` dan `book_id` sudah memiliki nilai unique yang sudah berupa integer terurut, namun hanya 812 buku yang akan digunakan. Ada kemungkinan ID tidak akan terurut lagi (terdapat angka urutan yang terlompat)"""

# Distribusi Rating
sns.countplot(x='rating', data=rating)
plt.title('Distribusi Rating')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""Insight : Mayoritas rating yang diberikan oleh pengguna berada pada rentang 3 hingga 5, dengan rating 4 yang mendominasi. Hal ini mungkin menunjukkan kecenderungan pengguna untuk memberikan rating yang lebih tinggi, atau bisa juga mencerminkan bias dalam cara sistem atau aplikasi memberikan rating."""

# Jumlah rating per user
user_rating_count = rating.groupby('user_id')['rating'].count()

plt.figure(figsize=(10, 4))
sns.histplot(user_rating_count, bins=50, kde=True)
plt.title('Jumlah Buku Dirating per User')
plt.xlabel('Jumlah rating')
plt.ylabel('Jumlah user')
plt.show()

"""Insight : Terlihat bahwa banyak pengguna hanya memberikan beberapa rating, dengan sebagian besar berada di sisi kiri grafik, menandakan bahwa ada banyak pengguna yang hanya memberikan rating untuk beberapa buku saja."""

# Jumlah rating per buku
book_rating_count = rating.groupby('book_id')['rating'].count()

plt.figure(figsize=(10, 4))
sns.histplot(book_rating_count, bins=50, kde=True)
plt.title('Jumlah Rating per Buku')
plt.xlabel('Jumlah rating')
plt.ylabel('Jumlah buku')
plt.show()

"""Insight : Banyak buku mendapatkan rating yang cukup banyak, dengan buku yang mendapat 100 rating menjadi kelompok terbesar.

## Data Preparation/Preprocessing

Data preparation/preprocessing dilakukan pada dataset untuk membersihkan, mengubah, dan mentransformasi data mentah menjadi format yang sesuai dan berkualitas tinggi agar dapat digunakan secara efektif oleh model machine learning.

### Variabel Books

Menghilangkan fitur `original_publication_year`, `average_rating`, dan `authors` karena tidak relevan untuk pembuatan model Collaborative Filtering
"""

pred_columns = [col for col in books.columns if col not in ['original_publication_year', 'authors', 'average_rating']]
books = books[pred_columns]

# Cek
books.info()

books.head(5)

"""Title tidak akan melalui proses encoding karena bukan untuk train model tetapi hanya sebagai tambahan informasi pada output rekomendasi."""

# Simpan mapping untuk nanti memanggil title berdasarkan hasil rekomendasi dari model
book_id_to_title = dict(zip(books['book_id'], books['title']))

"""### Variabel Rating

Mengatasi data duplikat
"""

# Untuk user memberi rating ke buku yang sama lebih dari sekali, ambil rata-rata ratingnya
rating = rating.groupby(['book_id', 'user_id'], as_index=False)['rating'].mean()

rating.head()

# Cek duplikasi data
# Memeriksa duplikasi data
duplikat_rating = rating.duplicated().sum()
print(f"Jumlah baris duplikat: {duplikat_rating}")
# Hanya duplikasi user_id dan book_id
dupe_count = rating.duplicated(subset=['user_id', 'book_id'], keep=False).sum()
print(f'Jumlah rating duplikat (user-buku): {dupe_count}')

# Cek jumlah data sekarang
rating.info()

rating.head()

"""### Merge Books dan Rating

Menggunakan inner join supaya buku yang diproses memiliki `title` yang tercantum dalam dataset
"""

# Merge berdasarkan book_id
merged = rating.merge(books[['book_id', 'title']],
                      on='book_id',
                      how='inner')

# Cek berapa jumlah judul yang NaN
print("Jumlah judul yang NaN :", merged['title'].isna().sum())

merged.info()

merged.head()

"""### Encoding

Karena pada proses _preprocessing_ banyak data yang dibuang, maka kemungkinan ID sudah tidak terurut dan memerlukan encoding
"""

# Cek
print("Max user_id:", merged['user_id'].max())
print("Unique users:", merged['user_id'].nunique())

print("Max book_id:", merged['book_id'].max())
print("Unique books:", merged['book_id'].nunique())

print(np.sort(merged['user_id'].unique()))

print(np.sort(merged['book_id'].unique()))

# Encode
merged['user_id'], user_index = pd.factorize(merged['user_id'])
merged['book_id'], book_index = pd.factorize(merged['book_id'])

"""Karena pada proses _preprocessing_ banyak `user_id` dan `book_id` yang hilang, hasilnya range ID tetap besar (misal user_id sampai 53424), tapi tidak semua ID digunakan lagi. Untuk menjaga agar memori tetap efisien dan model tidak lambat dilatih, maka lebih baik untuk dilakukan encoding terlebih dahulu.

### Splitting Data
"""

train_data, test_data = train_test_split(merged, test_size=0.2, random_state=42)

print("Jumlah data training:", len(train_data))
print("Jumlah data testing:", len(test_data))

train_data.head()

test_data.head()

"""### Scaling Data

Scaling dilakukan supaya fitur numerik lebih stabil dan _loss_ nantinya bisa terkontrol
"""

# Range rating sudah diketahui (0â€“5), jadi pembagian langsung
# Scale hanya kolom rating ke 0â€“1
train_data['rating'] = train_data['rating'] / 5.0
test_data['rating'] = test_data['rating'] / 5.0

train_data.head()

"""## Modelling Collaborative Filtering

### 1. Matrix Factorization model (SVD)

Dataset Loader
"""

class RatingDataset(Dataset):
    def __init__(self, df):
        self.users = torch.tensor(df['user_id'].values, dtype=torch.long)
        self.books = torch.tensor(df['book_id'].values, dtype=torch.long)
        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)

    def __len__(self):
        return len(self.ratings)

    def __getitem__(self, idx):
        return self.users[idx], self.books[idx], self.ratings[idx]

"""Model Matrix Factorization"""

class MatrixFactorization(nn.Module):
    def __init__(self, n_users, n_items, n_factors=64):
        super(MatrixFactorization, self).__init__()
        self.user_factors = nn.Embedding(n_users, n_factors)
        self.item_factors = nn.Embedding(n_items, n_factors)

    def forward(self, user, item):
        return (self.user_factors(user) * self.item_factors(item)).sum(1)

"""Train model SVD"""

# Tentukan jumlah user & item unik
n_users = merged['user_id'].nunique()
n_items = merged['book_id'].nunique()

# Inisialisasi model
model = MatrixFactorization(n_users, n_items)
loss_fn = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# DataLoader
train_dataset = RatingDataset(train_data)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Training loop
epochs = 10
model.train()
mf_losses = []

for epoch in range(epochs):
    total_loss = 0
    for user, item, rating in train_loader:
        optimizer.zero_grad()
        prediction = model(user, item)
        loss = loss_fn(prediction, rating)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    mf_losses.append(avg_loss)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

"""Evaluasi Model Matrix Factorization model (SVD)"""

# Dataset untuk test
test_dataset = RatingDataset(test_data)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

model.eval()
predictions = []
actuals = []

with torch.no_grad():
    for user, item, rating in test_loader:
        pred = model(user, item)
        predictions.extend(pred.numpy())
        actuals.extend(rating.numpy())

rmse = np.sqrt(mean_squared_error(actuals, predictions))
print(f"RMSE di test set: {rmse:.4f}")

# Simpan hasil evaluasi model Matrix Factorization
mf_rmse = rmse
print(f"Matrix Factorization RMSE disimpan: {mf_rmse:.4f}")

"""### 2. RecommenderNet (Deep Learning)

Bangun Model
"""

# Pastikan jumlah user dan book
n_users = merged['user_id'].nunique()
n_books = merged['book_id'].nunique()

# Inputs
user_input = Input(shape=(1,))
book_input = Input(shape=(1,))

# Embedding layer
user_embedding = Embedding(n_users, 32, embeddings_regularizer=l2(0.001))(user_input) # Tambah regularisasi pada embedding
book_embedding = Embedding(n_books, 32, embeddings_regularizer=l2(0.001))(book_input) # Tambah regularisasi pada embedding

# Flatten
user_vec = Flatten()(user_embedding)
book_vec = Flatten()(book_embedding)

# Concatenate
x = Concatenate()([user_vec, book_vec])

# Dense + Regularization
x = Dense(64, activation='relu', kernel_regularizer=l2(0.005))(x) # Tingkatkan L2, kurangi neuron
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
# Tingkatkan dropout menjadi 0.5 (cukup agresif)

x = Dense(32, activation='relu', kernel_regularizer=l2(0.005))(x) # Tingkatkan L2, kurangi neuron
x = Dropout(0.5)(x)
# Tingkatkan dropout lagi

# Output layer
output = Dense(1, activation='sigmoid')(x)  # karena rating distandarisasi ke 0â€“1

# Model
model = Model(inputs=[user_input, book_input], outputs=output)

# Compiler
# Coba turunkan learning rate sedikit jika masih terlalu agresif atau tidak stabil
model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['RootMeanSquaredError'])
model.summary()

# Early stopping callback
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True) # Agar tidak overfit

# Data
X_train = [train_data['user_id'].values, train_data['book_id'].values]
y_train = train_data['rating'].values
X_test = [test_data['user_id'].values, test_data['book_id'].values]
y_test = test_data['rating'].values

"""Train model RecommenderNet"""

history = model.fit(
    X_train, y_train,
    batch_size=64,
    epochs=20,
    validation_data=(X_test, y_test),
    callbacks=[early_stop],
    verbose=1
)

"""Evaluasi model RecommenderNet"""

y_pred = model.predict(X_test)
rec_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE on test set: {rec_rmse:.4f}")

"""## Evaluasi Model

Evaluasi model dilakukan untuk memilih model mana yang lebih baik untuk memberikan rekomendasi dan visualisasi training.
"""

print("\n===== Perbandingan Model =====")
print(f"Matrix Factorization RMSE:  {mf_rmse:.4f}")
print(f"RecommenderNet RMSE:        {rec_rmse:.4f}")

if mf_rmse < rec_rmse:
    print("Matrix Factorization memberikan hasil yang lebih baik (lebih rendah RMSE).")
else:
    print("RecommenderNet memberikan hasil yang lebih baik (lebih rendah RMSE).")

"""Maka, model rekomendasi yang disarankan untuk diimplementasikan adalah RecommenderNet

Visualisasi learning curve
"""

# Plot loss
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
plt.title('Learning Curve - Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True)

# Plot RMSE
plt.subplot(1, 2, 2)
plt.plot(history.history['RootMeanSquaredError'], label='Train RMSE', marker='o')
plt.plot(history.history['val_RootMeanSquaredError'], label='Validation RMSE', marker='o')
plt.title('Learning Curve - RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Grafik *Learning Curve* model RecommenderNet memberikan gambaran yang jelas mengenai proses pelatihan dan performa model pada data latih dan validasi:

a. Learning Curve - Loss (MSE)

Penurunan Tajam di Awal: Pada Epoch 0 hingga Epoch 1, train loss mengalami penurunan yang sangat signifikan, dari sekitar 0.21 menjadi sekitar 0.045. Hal ini mengindikasikan bahwa model dengan cepat berhasil mempelajari pola-pola utama dan mengurangi kesalahan prediksinya pada data pelatihan di tahap awal.
Stabilitas Setelah Epoch Awal: Setelah Epoch 1, baik train loss maupun validation loss menunjukkan perilaku yang relatif stabil dan cenderung mendatar. Ini menandakan bahwa model telah mencapai titik konvergensi di mana pembelajaran tambahan menghasilkan sedikit perbaikan dalam loss pada kedua set data.
Konsistensi Performa Validasi: Validation loss secara konsisten berada pada nilai yang sangat rendah dan stabil, seringkali sedikit di bawah train loss. Beberapa skenario dapat menyebabkan hal ini, seperti data validasi yang mungkin memiliki karakteristik sedikit lebih "mudah" diprediksi (underfit), atau pengaruh dari strategi regularisasi yang efektif.

b. Learning Curve - RMSE

Penurunan Konsisten dan Bertahap: Baik train RMSE maupun validation RMSE menunjukkan penurunan yang stabil dan konsisten dari Epoch 0 hingga Epoch 6. Ini adalah indikator positif bahwa model secara progresif mengurangi rata-rata kesalahan absolut prediksinya, baik pada data pelatihan maupun data validasi.
Performa Validasi yang Lebih Baik: Mirip dengan loss, validation RMSE juga secara konsisten sedikit lebih rendah atau setara dengan train RMSE. Hal ini menegaskan bahwa model mampu mempertahankan akurasi prediksi yang kuat pada data validasi sepanjang proses pelatihan.

### Inferensi menggunakan Model RecommenderNet
"""

def recommend_books_for_user(user_id, merged_df, book_df, model, top_n=5):
    if user_id not in merged_df['user_id'].values:
        return pd.DataFrame({'message': ['User ID not found']})

    read_books = merged_df[merged_df['user_id'] == user_id]['book_id'].tolist()
    unread_books = book_df[~book_df['book_id'].isin(read_books)]

    # Hindari index out-of-bounds di layer embedding
    unread_books = unread_books[unread_books['book_id'] < n_books]

    if unread_books.empty:
        return pd.DataFrame({'message': ['User has read all valid books']})

    book_ids = unread_books['book_id'].values.astype(np.int32).reshape(-1, 1)
    user_ids = np.full(len(book_ids), user_id, dtype=np.int32).reshape(-1, 1)

    preds = model.predict([user_ids, book_ids], verbose=0).flatten()
    top_indices = preds.argsort()[::-1][:top_n]

    recommended_books = unread_books.iloc[top_indices].copy()
    recommended_books['predicted_rating'] = preds[top_indices] * 5.0

    return recommended_books[['book_id', 'title', 'predicted_rating']]

# Buku rekomendasi untuk User dengan user_id 42
top_books = recommend_books_for_user(user_id=42, merged_df=merged, book_df=books, model=model, top_n=5)
print(top_books[['book_id', 'title', 'predicted_rating']])

"""Model merekomendasikan buku-buku berikut karena **memiliki `predicted_rating` tinggi**, yang menunjukkan bahwa **user kemungkinan besar akan menyukai buku-buku ini**:

| Judul Buku                                                 | Predicted Rating | Interpretasi                                                                      |
| ---------------------------------------------------------- | ---------------- | --------------------------------------------------------------------------------- |
| *The Elegant Universe: Superstrings, Hidden Dimensions...* | 4.20             | Sangat mungkin disukai user, buku populer dengan tema fisika dan teori string     |
| *The Broken Wings*                                         | 4.14             | Kemungkinan mirip dengan buku-buku yang sudah dibaca user sebelumnya              |
| *Atlas Shrugged*                                           | 4.08             | Novel klasik dengan tema filosofi dan ekonomi, cocok untuk pembaca berpikir       |
| *The Lord of the Rings: Weapons and Warfare*               | 4.06             | Buku dengan tema fantasi dan strategi, menarik bagi penggemar genre epik          |
| *Anthem*                                                   | 4.05             | Buku fiksi ilmiah dengan tema distopia, cocok untuk pembaca yang suka tema sosial |

---

### **Insight Umum**

* Semua skor prediksi berada di atas **4.0**, menandakan model memproyeksikan user ini sangat menyukai buku-buku dengan tema sains, filosofi, fantasi, dan fiksi ilmiah.
* Buku-buku ini **belum pernah dibaca oleh user** (hasil dari filtering `unread_books`).
* Rekomendasi ini sangat personal dan sesuai dengan minat serta pola bacaan user, bukan sekadar daftar buku populer.

---

Kalau mau, aku bisa bantu bikin versi lain seperti ringkasan pendek, slide, atau narasi presentasi!

"""